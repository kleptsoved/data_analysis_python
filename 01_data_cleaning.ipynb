{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f94acb33-ecb4-46bd-8119-dd4e34d76b14",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing\n",
    "\n",
    "## Overview\n",
    "This notebook covers essential steps for preparing raw data before analysis. The goal is to ensure that datasets are structured, free from inconsistencies, and ready for merging or visualization. The steps include:\n",
    "\n",
    "- Handling missing values\n",
    "- Formatting column names\n",
    "- Filtering and removing duplicates\n",
    "- Creating new features if necessary\n",
    "\n",
    "Data cleaning is the first step in any data analysis pipeline, ensuring the integrity and reliability of insights drawn from the dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d45881-cf4f-4299-bada-717618c567ed",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries\n",
    "The following libraries are used for data manipulation and visualization:\n",
    "- `pandas`: For working with structured data (tables, CSV, Excel)\n",
    "- `numpy`: For numerical computations\n",
    "- `matplotlib` & `seaborn`: For potential visual exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270e7f05-98e2-4aa1-93aa-8c8ef502dba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import chardet\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Force Jupyter Notebook to use all available horizontal space\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', 1000)          # Set width to a large number\n",
    "pd.set_option('display.max_colwidth', None)     # Show full column content if needed\n",
    "pd.set_option('display.float_format', lambda x: f\"{x:,.2f}\".replace(',', ' '))  # Format numbers with 2 decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac82063-2392-46cb-8a28-fd3156155afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the relative path of the directory where this script/notebook is located.\n",
    "script_dir = os.getcwd()  # or wherever your notebook is running\n",
    "\n",
    "# Go one level up (to the parent folder) and then into \"02 Data\".\n",
    "data_folder = os.path.join(script_dir, '..', '02 Data')\n",
    "input_path = os.path.join(data_folder, 'Original_data')\n",
    "output_path = os.path.join(data_folder, 'Processed_data')\n",
    "summary_report_path = os.path.join(output_path, 'summary_report.txt')\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "print(\"Data folder:\", data_folder)\n",
    "print(\"Input path:\", input_path)\n",
    "print(\"Output path:\", output_path)\n",
    "print(\"Summary report path:\", summary_report_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c3b308-213b-4c24-b4c4-48e9688effb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the input folder exists and list available files.\n",
    "if not os.path.exists(input_path):\n",
    "    print(f\"Error: The folder '{input_path}' does not exist. Please ensure the base folder is correct.\")\n",
    "else:\n",
    "    available_files = [f for f in os.listdir(input_path)]\n",
    "    print(\"Available files in the input folder:\")\n",
    "    for idx, f in enumerate(available_files, start=1):\n",
    "        print(f\"{idx}. {f}\")\n",
    "    \n",
    "    file_numbers_input = input(\n",
    "        \"\\nEnter the file numbers to process (comma-separated), or leave blank to process all files: \"\n",
    "    ).strip()\n",
    "    \n",
    "    if file_numbers_input:\n",
    "        try:\n",
    "            indices = [int(num.strip()) for num in file_numbers_input.split(',') if num.strip()]\n",
    "            # Validate indices and build the list of selected files.\n",
    "            files_list = [available_files[i-1] for i in indices if 1 <= i <= len(available_files)]\n",
    "            if not files_list:\n",
    "                print(\"No valid file numbers were entered.\")\n",
    "        except ValueError:\n",
    "            print(\"Error: Please enter valid numbers separated by commas.\")\n",
    "            files_list = []\n",
    "    else:\n",
    "        files_list = available_files\n",
    "\n",
    "    print(\"\\nFiles selected for processing:\", files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cba727-926f-4af3-a834-2d9f029d3858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting report generation\n",
    "summary_lines = []\n",
    "current_file = files_list[0]\n",
    "file_path = os.path.join(input_path, current_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9bec86-2e22-4fca-b9de-a85857f787cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_csv_with_delimiters(file_path, \n",
    "                                fallback_encodings=['latin1', 'ISO-8859-1', 'cp1252'], \n",
    "                                possible_delimiters=[',', ';', '\\t', '|']):\n",
    "    \"\"\"\n",
    "    Detects the file encoding, then previews the first 3 rows of the CSV using various delimiters.\n",
    "    Returns the detected encoding and a dictionary of previews keyed by delimiter.\n",
    "    \"\"\"\n",
    "    # Detect encoding using a sample from the file.\n",
    "    with open(file_path, 'rb') as f:\n",
    "        rawdata = f.read(100000)  # Read first 100k bytes\n",
    "    detection = chardet.detect(rawdata)\n",
    "    encoding = detection.get('encoding', 'utf-8')\n",
    "    print(f\"Detected encoding: {encoding}\\n\")\n",
    "    \n",
    "    previews = {}\n",
    "    for delim in possible_delimiters:\n",
    "        print(f\"Preview using delimiter {repr(delim)}:\")\n",
    "        try:\n",
    "            # Read only the first 3 rows for preview\n",
    "            df_preview = pd.read_csv(file_path, encoding=encoding, sep=delim, nrows=3)\n",
    "            previews[delim] = df_preview\n",
    "            print(df_preview)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed with delimiter {repr(delim)}. Error: {e}\")\n",
    "        print(\"-\" * 50 + \"\\n\")\n",
    "    return encoding, previews\n",
    "\n",
    "# Dictionary to store the DataFrames for each file.\n",
    "df = {}\n",
    "\n",
    "for file in files_list:\n",
    "    file_path = os.path.join(input_path, file)\n",
    "    if os.path.exists(file_path):\n",
    "        if file.endswith('.csv'):\n",
    "            print(f\"Processing CSV file: {file}\")\n",
    "            # Preview the CSV with different delimiters.\n",
    "            encoding, previews = preview_csv_with_delimiters(file_path)\n",
    "            \n",
    "            # List the delimiter options with numbers.\n",
    "            possible_delimiters = [',', ';', '\\t', '|']\n",
    "            print(\"Select the correct delimiter from the options below:\")\n",
    "            for idx, delim in enumerate(possible_delimiters, start=1):\n",
    "                print(f\"{idx}. {repr(delim)}\")\n",
    "            \n",
    "            # Ask the user to choose the correct delimiter by number.\n",
    "            while True:\n",
    "                try:\n",
    "                    selected_number = int(input(\"Enter the number corresponding to the correct delimiter: \"))\n",
    "                    if 1 <= selected_number <= len(possible_delimiters):\n",
    "                        selected_delim = possible_delimiters[selected_number - 1]\n",
    "                        break\n",
    "                    else:\n",
    "                        print(\"Invalid number. Please choose a valid option.\")\n",
    "                except ValueError:\n",
    "                    print(\"Please enter a valid integer.\")\n",
    "            \n",
    "            # Now load the full CSV using the selected delimiter.\n",
    "            try:\n",
    "                df_csv = pd.read_csv(file_path, encoding=encoding, sep=selected_delim)\n",
    "                df[file] = df_csv\n",
    "                print(f\"Successfully loaded '{file}' with delimiter {repr(selected_delim)} \" \\\n",
    "                      f\"(rows: {df_csv.shape[0]}, columns: {df_csv.shape[1]})\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading CSV file {file} with delimiter {repr(selected_delim)}: {e}\")\n",
    "                continue\n",
    "        elif file.endswith('.pkl'):\n",
    "            try:\n",
    "                df[file] = pd.read_pickle(file_path)\n",
    "                print(f\"Loaded pickle file '{file}' (rows: {df[file].shape[0]}, columns: {df[file].shape[1]})\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading pickle file {file}: {e}\")\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"Skipping unsupported file format: {file}\")\n",
    "            continue\n",
    "        \n",
    "        print(\"=\" * 100 + \"\\n\")\n",
    "    else:\n",
    "        print(f\"File {file} not found and will be skipped.\")\n",
    "\n",
    "# Logging details.\n",
    "report_details = [f\"File: {current_file}\"]\n",
    "report_details.append(f\"Total loaded files: {len(df)}\")\n",
    "modifications = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ee9f50-f85d-4feb-905e-a8ecc86642ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# present all imported DataFrames\n",
    "\n",
    "for file_name, data in df.items():\n",
    "    html = data.to_html(max_rows=2, max_cols=30)\n",
    "    display(HTML(f'<h4>{file_name}</h4><div style=\"overflow-x: auto; width:100%;\">{html}</div>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0108de24-ef9b-425a-9370-613b52f3cacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[current_file].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52909e90-6b78-457d-be7e-9b4b3254aed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[current_file].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42937cd3-8a68-4989-82b5-1d6066dec31e",
   "metadata": {},
   "source": [
    "## Handling Missing Values\n",
    "Missing data can affect analysis accuracy. This section explores strategies such as:\n",
    "- Removing rows/columns with excessive missing values\n",
    "- Imputing missing values based on statistical methods\n",
    "- Using placeholders for unknown values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db376776-3205-4603-a0cf-a7c94c90fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Customization – Exclude Columns\n",
    "df[current_file].head()\n",
    "data = df[current_file]\n",
    "data.head()\n",
    "exclude_cols_input = input(\"\\nEnter columns to exclude (comma-separated), or press Enter to skip: \").strip()\n",
    "if exclude_cols_input:\n",
    "    exclude_cols = [col.strip() for col in exclude_cols_input.split(',') if col.strip()]\n",
    "    data = data.drop(columns=exclude_cols, errors='ignore')\n",
    "    modifications.append(f\"Excluded columns: {', '.join(exclude_cols)}\")\n",
    "    print(f\"Columns excluded: {exclude_cols}\")\n",
    "else:\n",
    "    print(\"No columns were excluded.\")\n",
    "\n",
    "# Update the DataFrame in the dictionary.\n",
    "df[current_file] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9ccd0d-e9d0-43cd-b5b7-deb5e76f4316",
   "metadata": {},
   "source": [
    "## Formatting and Standardizing Column Names\n",
    "To ensure consistency across datasets, column names are standardized:\n",
    "- Converted to lowercase\n",
    "- Replacing spaces with underscores\n",
    "- Removing special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1338824-ed2e-4497-be04-615c6499cfc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data Customization – Rename Columns\n",
    "while True:\n",
    "    print(\"\\nCurrent column names:\")\n",
    "    print(list(data.columns))\n",
    "    col_to_rename = input(\"Enter column name to rename, or press Enter to stop renaming: \").strip()\n",
    "    if not col_to_rename:\n",
    "        break\n",
    "    if col_to_rename in data.columns:\n",
    "        new_name = input(f\"Enter new name for column '{col_to_rename}': \").strip()\n",
    "        data.rename(columns={col_to_rename: new_name}, inplace=True)\n",
    "        modifications.append(f\"Renamed column '{col_to_rename}' to '{new_name}'\")\n",
    "        print(f\"Renamed '{col_to_rename}' to '{new_name}'\")\n",
    "    else:\n",
    "        print(f\"Column '{col_to_rename}' not found.\")\n",
    "        \n",
    "df[current_file] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e807c8c2-bd0a-490e-bace-d0296dffc2f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example function to clean currency-like strings and convert to float\n",
    "def parse_currency_string(value):\n",
    "    \"\"\"\n",
    "    Attempts to remove currency symbols, spaces, and convert commas to dots,\n",
    "    then parses the result as a float. Leaves NaNs as NaN.\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return value  # Keep NaNs as is\n",
    "    # Remove '$' if present (or other currency symbols)\n",
    "    value = str(value).replace('$', '')\n",
    "    # Remove spaces\n",
    "    value = value.replace(' ', '')\n",
    "    # Replace comma with dot (assuming comma decimal format)\n",
    "    value = value.replace(',', '.')\n",
    "    # Convert to float\n",
    "    return float(value)\n",
    "\n",
    "# List of available data types (key + explanation)\n",
    "available_dtypes = [\n",
    "    ('int', \"Integer (no decimal part). E.g. 42\"),\n",
    "    ('float', \"Floating-point number (decimal allowed). E.g. 3.14\"),\n",
    "    ('str', \"String (text). E.g. 'Hello world'\"),\n",
    "    ('bool', \"Boolean (True or False)\"),\n",
    "    ('datetime64[ns]', \"Date and time in Pandas datetime format\"),\n",
    "    ('category', \"Categorical data (saves memory if repetitive values)\"),\n",
    "]\n",
    "\n",
    "# Data Customization – Change Data Types by column number\n",
    "while True:\n",
    "    print(\"\\nCurrent data types:\")\n",
    "    print(data.dtypes)\n",
    "    \n",
    "    # Number each column\n",
    "    columns_list = list(data.columns)\n",
    "    print(\"\\nColumns:\")\n",
    "    for i, col in enumerate(columns_list, start=1):\n",
    "        print(f\"{i}. {col} (current dtype: {data[col].dtype})\")\n",
    "    \n",
    "    # Ask user which column to convert by number\n",
    "    col_number_input = input(\"\\nEnter the column number to change data type, or press Enter to stop:\\n\").strip()\n",
    "    if not col_number_input:\n",
    "        break  # Stop if user presses Enter without a choice\n",
    "    \n",
    "    try:\n",
    "        col_number = int(col_number_input)\n",
    "        if 1 <= col_number <= len(columns_list):\n",
    "            col_to_cast = columns_list[col_number - 1]\n",
    "        else:\n",
    "            print(\"Invalid column number. Please try again.\")\n",
    "            continue\n",
    "    except ValueError:\n",
    "        print(\"Please enter a valid integer for the column number.\")\n",
    "        continue\n",
    "    \n",
    "    # Show available data types by number\n",
    "    print(\"\\nAvailable data types:\")\n",
    "    for idx, (dt_key, dt_expl) in enumerate(available_dtypes, start=1):\n",
    "        print(f\"{idx}. {dt_key} - {dt_expl}\")\n",
    "    \n",
    "    # Ask user for the new data type by number\n",
    "    dtype_choice = input(f\"\\nEnter the number corresponding to the desired data type for '{col_to_cast}':\\n\").strip()\n",
    "    try:\n",
    "        dtype_choice_num = int(dtype_choice)\n",
    "        if 1 <= dtype_choice_num <= len(available_dtypes):\n",
    "            new_dtype = available_dtypes[dtype_choice_num - 1][0]\n",
    "        else:\n",
    "            print(\"Invalid number. Please choose a valid option.\")\n",
    "            continue\n",
    "    except ValueError:\n",
    "        print(\"Please enter a valid integer.\")\n",
    "        continue\n",
    "    \n",
    "    # Try direct casting first\n",
    "    try:\n",
    "        data[col_to_cast] = data[col_to_cast].astype(new_dtype)\n",
    "        modifications.append(f\"Changed data type of '{col_to_cast}' to {new_dtype}\")\n",
    "        print(f\"Changed data type of '{col_to_cast}' to {new_dtype}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to change data type of '{col_to_cast}' to {new_dtype}: {e}\")\n",
    "        \n",
    "        # Attempt auto-clean only if user wants int or float\n",
    "        if new_dtype in [\"int\", \"float\"]:\n",
    "            print(f\"\\nAttempting to auto-clean the '{col_to_cast}' column for {new_dtype} conversion...\")\n",
    "\n",
    "            try:\n",
    "                # 1. Clean the column (remove currency symbols, fix decimal separators, etc.)\n",
    "                temp_col = data[col_to_cast].apply(parse_currency_string)\n",
    "                \n",
    "                # 2. Handle missing values before casting to int/float\n",
    "                missing_count = temp_col.isna().sum()\n",
    "                if missing_count > 0:\n",
    "                    print(f\"Found {missing_count} missing (NaN) values in '{col_to_cast}'.\")\n",
    "                    print(\"How would you like to handle these missing values?\")\n",
    "                    print(\"1. Drop rows with missing values\")\n",
    "                    print(\"2. Fill missing values with 0\")\n",
    "                    print(\"3. Fill missing values with a custom value\")\n",
    "                    print(\"4. Leave them as NaN (only works for float)\")\n",
    "                    mv_choice = input(\"Enter the number of your choice (1/2/3/4): \").strip()\n",
    "                    \n",
    "                    if mv_choice == \"1\":\n",
    "                        temp_col = temp_col.dropna()\n",
    "                        print(\"Dropped rows with missing values.\")\n",
    "                    elif mv_choice == \"2\":\n",
    "                        temp_col = temp_col.fillna(0)\n",
    "                        print(\"Filled missing values with 0.\")\n",
    "                    elif mv_choice == \"3\":\n",
    "                        fill_val = input(\"Enter the value to fill missing values: \")\n",
    "                        # Convert fill_val to the appropriate numeric type\n",
    "                        if new_dtype == \"float\":\n",
    "                            fill_val = float(fill_val)\n",
    "                        elif new_dtype == \"int\":\n",
    "                            fill_val = int(float(fill_val))  # in case user typed \"3.0\"\n",
    "                        temp_col = temp_col.fillna(fill_val)\n",
    "                        print(f\"Filled missing values with '{fill_val}'.\")\n",
    "                    elif mv_choice == \"4\":\n",
    "                        if new_dtype == \"float\":\n",
    "                            print(\"Leaving missing values as NaN.\")\n",
    "                        else:\n",
    "                            print(\"Cannot leave NaN if converting to int. Attempting fill with 0.\")\n",
    "                            temp_col = temp_col.fillna(0)\n",
    "                    else:\n",
    "                        print(\"Invalid choice. Leaving missing values as NaN for now.\")\n",
    "                \n",
    "                # 3. Convert to float or int as requested\n",
    "                if new_dtype == \"int\":\n",
    "                    temp_col = temp_col.astype(int)\n",
    "                else:\n",
    "                    temp_col = temp_col.astype(float)\n",
    "                \n",
    "                # 4. Show side-by-side comparison for first 5 rows\n",
    "                comparison_df = pd.DataFrame({\n",
    "                    \"original\": data[col_to_cast].head(5),\n",
    "                    \"converted\": temp_col.head(5)\n",
    "                })\n",
    "                \n",
    "                print(\"\\nPreview of original vs. converted values (first 5 rows):\")\n",
    "                print(comparison_df)\n",
    "                \n",
    "                # 5. Prompt user confirmation\n",
    "                confirm = input(\"\\nDoes this look correct? (y/n): \").strip().lower()\n",
    "                if confirm == \"y\":\n",
    "                    # If we dropped rows, we need to align the main DataFrame with temp_col’s index\n",
    "                    data = data.reindex(temp_col.index)  # In case some rows were dropped\n",
    "                    data[col_to_cast] = temp_col\n",
    "                    modifications.append(f\"Auto-cleaned and changed data type of '{col_to_cast}' to {new_dtype}\")\n",
    "                    print(f\"Successfully auto-cleaned and changed data type of '{col_to_cast}' to {new_dtype}\")\n",
    "                else:\n",
    "                    print(\"No changes applied.\")\n",
    "            \n",
    "            except Exception as e2:\n",
    "                print(f\"Auto-cleaning also failed: {e2}\")\n",
    "        else:\n",
    "            print(\"Auto-cleaning is only implemented for int or float conversions.\")\n",
    "\n",
    "# Finally, store the updated DataFrame back if needed\n",
    "df[current_file] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df8c273-79a8-458f-9373-714bfd718cb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Checking for missing values in the dataset\n",
    "print(\"\\nChecking for missing values...\")\n",
    "missing_summary = data.isnull().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0]\n",
    "rows_before = len(data)\n",
    "\n",
    "if not missing_summary.empty:\n",
    "    print(\"Columns with missing values and their counts:\")\n",
    "    print(missing_summary)\n",
    "    \n",
    "    print(\"\\nColumn statistics:\")\n",
    "    display(data.describe())\n",
    "    \n",
    "    # Loop through each column that has missing values\n",
    "    for column in missing_summary.index:\n",
    "        # Display the first 5 rows where the current column has missing values\n",
    "        print(f\"\\nFirst 5 rows where '{column}' is missing:\")\n",
    "        display(data[data[column].isnull()].head())\n",
    "        \n",
    "        fill_method = input(\n",
    "            f\"Enter method to handle missing values for '{column}' \"\n",
    "            \"(mean, median, drop, or custom value), or press Enter to skip: \"\n",
    "        ).strip()\n",
    "        \n",
    "        if fill_method == 'mean':\n",
    "            data[column] = data[column].fillna(data[column].mean())\n",
    "            modifications.append(f\"Filled missing values in '{column}' with mean\")\n",
    "        \n",
    "        elif fill_method == 'median':\n",
    "            data[column] = data[column].fillna(data[column].median())\n",
    "            modifications.append(f\"Filled missing values in '{column}' with median\")\n",
    "        \n",
    "        elif fill_method == 'drop':\n",
    "            data.dropna(subset=[column], inplace=True)\n",
    "            modifications.append(f\"Dropped rows with missing values in '{column}'\")\n",
    "        \n",
    "        elif fill_method:\n",
    "            try:\n",
    "                if data[column].dtype.kind in 'fc':  # numeric types\n",
    "                    value = float(fill_method)\n",
    "                else:\n",
    "                    value = fill_method\n",
    "                \n",
    "                data[column] = data[column].fillna(value)\n",
    "                modifications.append(f\"Filled missing values in '{column}' with custom value: {value}\")\n",
    "            \n",
    "            except ValueError:\n",
    "                print(f\"Invalid custom value for '{column}', skipping...\")\n",
    "else:\n",
    "    print(\"No missing values detected.\")\n",
    "\n",
    "rows_after = len(data)\n",
    "report_details.append(f\"Rows dropped due to missing values: {rows_before - rows_after}\")\n",
    "\n",
    "df[current_file] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c53565-6c33-4bd0-b583-f597dd8099c3",
   "metadata": {},
   "source": [
    "## Removing Duplicates\n",
    "Duplicate records can distort analysis results. This step identifies and removes any redundant entries to maintain dataset integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e518e445-76a6-4bca-8ce8-dec5de8836b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate Row Management\n",
    "\n",
    "print(\"\\nChecking for duplicate rows...\")\n",
    "duplicates = data.duplicated().sum()\n",
    "print(f\"Found {duplicates} duplicate rows.\")\n",
    "report_details.append(f\"Number of duplicate rows: {duplicates}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(\"Preview of duplicate rows:\")\n",
    "    display(data[data.duplicated()].head())\n",
    "    drop_dup = input(\"Do you want to drop duplicates? (yes/no): \").strip().lower()\n",
    "    if drop_dup == 'yes':\n",
    "        rows_before_dup = len(data)\n",
    "        data.drop_duplicates(inplace=True)\n",
    "        rows_after_dup = len(data)\n",
    "        modifications.append(\"Dropped duplicate rows\")\n",
    "        report_details.append(f\"Rows dropped due to duplicates: {rows_before_dup - rows_after_dup}\")\n",
    "        print(\"Duplicates dropped.\")\n",
    "    else:\n",
    "        print(\"Duplicates not dropped.\")\n",
    "        \n",
    "df[current_file] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056396d7-aaa2-4ce6-80c1-b12ade014be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Detection\n",
    "\n",
    "print(\"\\nDetecting outliers in numeric columns...\")\n",
    "outliers_info = {}\n",
    "for col in data.select_dtypes(include=['number']).columns:\n",
    "    q1 = data[col].quantile(0.25)\n",
    "    q3 = data[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    outlier_rows = data[(data[col] < lower_bound) | (data[col] > upper_bound)]\n",
    "    if not outlier_rows.empty:\n",
    "        outliers_info[col] = len(outlier_rows)\n",
    "        print(f\"Column '{col}': {len(outlier_rows)} outlier rows detected.\")\n",
    "        display(outlier_rows.head())\n",
    "        \n",
    "if outliers_info:\n",
    "    modifications.append(f\"Outliers detected: {outliers_info}\")\n",
    "    report_details.append(\"Outlier detection completed.\")\n",
    "else:\n",
    "    report_details.append(\"No outliers detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2ec527-2de1-42e5-b883-972916fbd00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efabc2e4-f3dd-443f-ac48-600dd5ec1e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[current_file].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5230bda3-c412-4cbb-972f-67100db9cbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask user for file format preference: CSV or pkl\n",
    "file_format = input(\"Enter desired output file format (csv or pkl): \").strip().lower()\n",
    "while file_format not in ['csv', 'pkl']:\n",
    "    file_format = input(\"Invalid format. Please enter 'csv' or 'pkl': \").strip().lower()\n",
    "\n",
    "# Prompt the user for the file name (without extension)\n",
    "output_filename = input(\"Enter the desired file name (without extension): \").strip()\n",
    "output_file = os.path.join(output_path, f\"{output_filename}.{file_format}\")\n",
    "\n",
    "# Save the processed DataFrame in the selected format\n",
    "if file_format == 'csv':\n",
    "    data.to_csv(output_file, index=False)\n",
    "elif file_format == 'pkl':\n",
    "    data.to_pickle(output_file)\n",
    "\n",
    "print(f\"\\n✅ Processed file saved to: {output_file}\")\n",
    "report_details.append(f\"Processed file saved to: {output_file}\")\n",
    "report_details.append(f\"Total rows in the exported file: {len(data)}\")\n",
    "\n",
    "# Update the stored data frame for the current file\n",
    "df[current_file] = data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
