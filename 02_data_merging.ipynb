{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19d5abc6-c7c8-4743-b597-6641c4631256",
   "metadata": {},
   "source": [
    "# Data Merging & Integration\n",
    "\n",
    "## Overview\n",
    "Combining multiple datasets is a key step in data analysis. This notebook demonstrates how to efficiently merge datasets using different join techniques, ensuring consistency and completeness in the final dataset.\n",
    "\n",
    "Key steps include:\n",
    "- Understanding different types of joins (`inner`, `left`, `right`, `outer`)\n",
    "- Merging datasets based on common keys\n",
    "- Handling duplicate records after merging\n",
    "- Checking for inconsistencies in merged datasets\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeb3e96-30b2-4608-a0e7-a87f6912cd76",
   "metadata": {},
   "source": [
    "## Importing Required Libraries\n",
    "This section loads the necessary libraries for merging datasets efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aed4234-ce31-47c8-97a6-12296c87f708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Force Jupyter Notebook to use all available horizontal space\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', 1000)          # Set width to a large number\n",
    "pd.set_option('display.max_colwidth', None)     # Show full column content if needed\n",
    "pd.set_option('display.float_format', '{:,.2f}'.format)  # Format numbers with 2 decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943a3538-1549-4073-8ca9-dc17e8db76f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the relative path of the directory where this script/notebook is located.\n",
    "script_dir = os.getcwd()  # or wherever your notebook is running\n",
    "\n",
    "# Go one level up (to the parent folder) and then into \"02 Data\".\n",
    "data_folder = os.path.join(script_dir, '..', '02 Data')\n",
    "input_path = os.path.join(data_folder, 'Processed_data')\n",
    "output_path = os.path.join(data_folder, 'Merged_data')\n",
    "summary_report_path = os.path.join(output_path, 'summary_report.txt')\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "print(\"Data folder:\", data_folder)\n",
    "print(\"Input path:\", input_path)\n",
    "print(\"Output path:\", output_path)\n",
    "print(\"Summary report path:\", summary_report_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f584da-1429-477f-8ffb-35ec676e1aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the input folder exists and list available files.\n",
    "if not os.path.exists(input_path):\n",
    "    print(f\"Error: The folder '{input_path}' does not exist. Please ensure the base folder is correct.\")\n",
    "else:\n",
    "    available_files = [f for f in os.listdir(input_path)]\n",
    "    print(\"Available files in the input folder:\")\n",
    "    for idx, f in enumerate(available_files, start=1):\n",
    "        print(f\"{idx}. {f}\")\n",
    "    \n",
    "    file_numbers_input = input(\n",
    "        \"\\nEnter the file numbers to process (comma-separated), or leave blank to process all files: \"\n",
    "    ).strip()\n",
    "    \n",
    "    if file_numbers_input:\n",
    "        try:\n",
    "            indices = [int(num.strip()) for num in file_numbers_input.split(',') if num.strip()]\n",
    "            # Validate indices and build the list of selected files.\n",
    "            selected_files = [available_files[i-1] for i in indices if 1 <= i <= len(available_files)]\n",
    "            if not selected_files:\n",
    "                print(\"No valid file numbers were entered.\")\n",
    "        except ValueError:\n",
    "            print(\"Error: Please enter valid numbers separated by commas.\")\n",
    "            selected_filest = []\n",
    "    else:\n",
    "        selected_files = available_files\n",
    "\n",
    "    print(\"\\nFiles selected for processing:\", selected_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3309250a-d838-4442-9d34-b80ef5d5186e",
   "metadata": {},
   "source": [
    "## Loading Datasets\n",
    "Here, we import the datasets that will be merged. Each dataset contains unique attributes that will be combined based on common identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0781ecac-6d71-438e-a795-01b58bd799bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_file = selected_files[0]\n",
    "file_path = os.path.join(input_path, current_file)\n",
    "\n",
    "# Load files into DataFrames and display heads\n",
    "df = {}\n",
    "for file in selected_files:\n",
    "    file_path = os.path.join(input_path, file)\n",
    "    if os.path.exists(file_path):\n",
    "        # Determine file format and load accordingly\n",
    "        if file.endswith('.csv'):\n",
    "            df[file] = pd.read_csv(file_path)\n",
    "        elif file.endswith('.pkl'):\n",
    "            df[file] = pd.read_pickle(file_path)\n",
    "        else:\n",
    "            print(f\"Skipping unsupported file format: {file}\")\n",
    "            continue  # Skip unsupported formats\n",
    "        \n",
    "        print(f\"Loaded file:\\n{file} (rows: {df[file].shape[0]}, columns: {df[file].shape[1]})\")\n",
    "        # print(df[file].head())  # Display first few rows\n",
    "        print(\"=\"*100 + \"\\n\")\n",
    "    else:\n",
    "        print(f\"File {file} not found and will be skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742949b4-5923-4550-b2d4-38f8ea5005f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt user to choose operation type\n",
    "operation_type = input(\"Choose operation (merge/concatenate): \").strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d552940-a597-436b-96ca-cd5eb0bf4e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# present all imported DataFrames\n",
    "\n",
    "for file_name, data in df.items():\n",
    "    html = data.to_html(max_rows=2, max_cols=30)\n",
    "    display(HTML(f'<h4>{file_name}</h4><div style=\"overflow-x: auto; width:100%;\">{html}</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98cd177-9c29-4c16-86bd-154c0ecdd3df",
   "metadata": {},
   "source": [
    "## Types of Joins in Pandas\n",
    "The following merging methods are used:\n",
    "- **Inner Join**: Includes only matching records in both datasets\n",
    "- **Left Join**: Includes all records from the left dataset and matching ones from the right\n",
    "- **Right Join**: Includes all records from the right dataset and matching ones from the left\n",
    "- **Outer Join**: Includes all records from both datasets, filling in missing values where necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b4d08d-071d-4275-98d9-9b33e95c6552",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if operation_type == \"merge\":\n",
    "    common_key = input(\"Enter the key column to use for merging: \").strip()\n",
    "    merge_type = input(\"Choose merge type (inner, outer, left, right): \").strip()\n",
    "    \n",
    "    # Initialize merged DataFrame\n",
    "    merged_df = None\n",
    "\n",
    "    for idx, (file, df_subset) in enumerate(df.items()):\n",
    "        if idx == 0:\n",
    "            merged_df = df_subset  # First file remains as base\n",
    "        else:\n",
    "            merged_df = merged_df.merge(df_subset, on=common_key, how=merge_type, indicator=True)\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Check merge effectiveness\n",
    "if '_merge' in merged_df.columns:\n",
    "    print(\"\\nMerged dataset preview:\")\n",
    "    \n",
    "    # Convert DataFrame to HTML with horizontal scrolling\n",
    "    html = merged_df.to_html(max_rows=2, max_cols=20)  # Adjust limits as needed\n",
    "    display(HTML(f'<div style=\"overflow-x: auto; width:100%;\">{html}</div>'))\n",
    "    \n",
    "    # Calculate merge statistics\n",
    "    merge_counts = merged_df['_merge'].value_counts()\n",
    "    merge_percentages = merged_df['_merge'].value_counts(normalize=True) * 100\n",
    "    \n",
    "    # Display merge match percentage along with count\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"\\nMerge match percentage and count:\\n\")\n",
    "    merge_stats = pd.DataFrame({'Count': merge_counts, 'Percentage (%)': merge_percentages})\n",
    "    print(merge_stats)\n",
    "\n",
    "    # Keep the merge flag for analysis but drop if not needed\n",
    "    # merged_df.drop(columns=['_merge'], inplace=True)  # Uncomment to remove after check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95e0a5a-fe46-409e-99dd-d824dbea0769",
   "metadata": {},
   "outputs": [],
   "source": [
    "if operation_type == \"concatenate\":\n",
    "    # Concatenate all selected DataFrames\n",
    "    concatenated_df = pd.concat(df.values(), ignore_index=True)\n",
    "\n",
    "    # Display basic info\n",
    "    print(f\"Concatenated DataFrame (rows: {concatenated_df.shape[0]}, columns: {concatenated_df.shape[1]})\")\n",
    "    print(concatenated_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553627e9-4f75-4b22-afe9-59550715033d",
   "metadata": {},
   "source": [
    "## Post-Merge Data Integrity Check\n",
    "After merging, it’s important to check:\n",
    "- The total number of rows and columns\n",
    "- Whether duplicate records have been created\n",
    "- If any data is missing due to the join operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d6cb85-52d1-47ca-9ffe-d479dd706998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only the main imported dataset's row count\n",
    "main_file = selected_files[0]\n",
    "total_imported_rows = df[main_file].shape[0]\n",
    "\n",
    "if operation_type == \"merge\":\n",
    "    exported_rows = merged_df.shape[0]\n",
    "elif operation_type == \"concatenate\":\n",
    "    exported_rows = concatenated_df.shape[0]\n",
    "\n",
    "# Display the row counts\n",
    "print(f\"Total rows in main imported dataset: {total_imported_rows}\")  \n",
    "print(f\"Final dataset rows: {exported_rows}\\n\")  \n",
    "\n",
    "# Warn if there is a mismatch\n",
    "if total_imported_rows != exported_rows:\n",
    "    print(f\"⚠️ Warning: Shape mismatch! \\nMain imported rows: {total_imported_rows} \\nExported rows: {exported_rows}\")\n",
    "else:\n",
    "    print(f\"✅ Shape consistency check passed:\\nMain imported rows: {total_imported_rows} \\nExported rows: {exported_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6cfd4f-30aa-4463-9319-df1427f45d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask user for file format preference: CSV or pkl\n",
    "file_format = input(\"Enter desired output file format (csv or pkl): \").strip().lower()\n",
    "while file_format not in ['csv', 'pkl']:\n",
    "    file_format = input(\"Invalid format. Please enter 'csv' or 'pkl': \").strip().lower()\n",
    "\n",
    "# Prompt the user for the file name (without extension)\n",
    "output_filename = input(\"Enter the desired file name (without extension): \").strip()\n",
    "output_file = os.path.join(output_path, f\"{output_filename}.{file_format}\")\n",
    "\n",
    "# Save the processed DataFrame in the selected format\n",
    "# to be updated with a list of available data set in memory\n",
    "if file_format == 'csv':\n",
    "    data.to_csv(output_file, index=False)\n",
    "elif file_format == 'pkl':\n",
    "    data.to_pickle(output_file)\n",
    "\n",
    "print(f\"\\n✅ Processed file saved to: {output_file}\")\n",
    "report_details.append(f\"Processed file saved to: {output_file}\")\n",
    "report_details.append(f\"Total rows in the exported file: {len(data)}\")\n",
    "\n",
    "# Update the stored data frame for the current file\n",
    "df[current_file] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d007a0-cf4d-4b09-8a35-3c1b7c86bf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "with open(summary_report_path, 'w') as report:\n",
    "    report.write(\"Data Processing Report\\n\")\n",
    "    report.write(\"============================\\n\")\n",
    "    report.write(f\"Files processed: {', '.join(selected_files)}\\n\")\n",
    "    report.write(f\"Operation type: {operation_type}\\n\")\n",
    "    \n",
    "    if operation_type == \"merge\":\n",
    "        report.write(f\"Key column: {common_key}\\n\")\n",
    "        report.write(f\"Merge type: {merge_type}\\n\")\n",
    "        if '_merge' in merged_df.columns:\n",
    "            report.write(\"Merge match percentage:\\n\")\n",
    "            report.write(str(merge_counts) + \"\\n\")\n",
    "    \n",
    "    report.write(f\"File saved at: {output_file}\\n\")\n",
    "\n",
    "print(f\"Summary report saved: {summary_report_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
